{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         1|       Alice|        HR|  4000|\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         3|     Charlie|        HR|  4500|\n",
      "|         4|       David|        IT|  6000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|         6|       Frank|        IT|  7000|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|         8|       Heidi|   Finance|  5000|\n",
      "|         9|        Ivan|   Finance|  4500|\n",
      "|        10|        Judy|     Sales|  3000|\n",
      "|        11|       Kevin|     Sales|  3200|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        13|     Mallory| Marketing|  4000|\n",
      "|        14|        Niaj| Marketing|  4500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Sample employee data\n",
    "data = [\n",
    "    Row(EmployeeID=1, EmployeeName=\"Alice\", Department=\"HR\", Salary=4000),\n",
    "    Row(EmployeeID=2, EmployeeName=\"Bob\", Department=\"HR\", Salary=5000),\n",
    "    Row(EmployeeID=3, EmployeeName=\"Charlie\", Department=\"HR\", Salary=4500),\n",
    "    Row(EmployeeID=4, EmployeeName=\"David\", Department=\"IT\", Salary=6000),\n",
    "    Row(EmployeeID=5, EmployeeName=\"Eve\", Department=\"IT\", Salary=7500),\n",
    "    Row(EmployeeID=6, EmployeeName=\"Frank\", Department=\"IT\", Salary=7000),\n",
    "    Row(EmployeeID=7, EmployeeName=\"Grace\", Department=\"Finance\", Salary=5500),\n",
    "    Row(EmployeeID=8, EmployeeName=\"Heidi\", Department=\"Finance\", Salary=5000),\n",
    "    Row(EmployeeID=9, EmployeeName=\"Ivan\", Department=\"Finance\", Salary=4500),\n",
    "    Row(EmployeeID=10, EmployeeName=\"Judy\", Department=\"Sales\", Salary=3000),\n",
    "    Row(EmployeeID=11, EmployeeName=\"Kevin\", Department=\"Sales\", Salary=3200),\n",
    "    Row(EmployeeID=12, EmployeeName=\"Laura\", Department=\"Sales\", Salary=3500),\n",
    "    Row(EmployeeID=13, EmployeeName=\"Mallory\", Department=\"Marketing\", Salary=4000),\n",
    "    Row(EmployeeID=14, EmployeeName=\"Niaj\", Department=\"Marketing\", Salary=4500),\n",
    "    Row(EmployeeID=15, EmployeeName=\"Oscar\", Department=\"Marketing\", Salary=4800)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        14|        Niaj| Marketing|  4500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\"\n",
    "SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary \n",
    "FROM Employees e\n",
    "INNER JOIN (\n",
    "    SELECT Department, AVG(Salary) AS avg_salary \n",
    "    FROM Employees \n",
    "    GROUP BY Department\n",
    ") avgEmpSal\n",
    "ON e.Department = avgEmpSal.Department\n",
    "WHERE e.Salary > avgEmpSal.avg_salary\n",
    "AND e.Salary < (SELECT MAX(avg_salary) FROM (\n",
    "    SELECT AVG(Salary) AS avg_salary \n",
    "    FROM Employees \n",
    "    GROUP BY Department\n",
    ") tempAvg)\n",
    "\"\"\")\n",
    "\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         1|       Alice|        HR|  4000|\n",
      "|         4|       David|        IT|  6000|\n",
      "|         9|        Ivan|   Finance|  4500|\n",
      "|        13|     Mallory| Marketing|  4000|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH DepartmentAverages AS (\n",
    "    SELECT Department, AVG(Salary) AS avg_salary FROM Employees GROUP BY Department\n",
    "    \n",
    "    \n",
    "), EmployeeDetails AS (\n",
    "    SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary, d.avg_salary AS dept_avg_salary\n",
    "    FROM Employees e\n",
    "    JOIN DepartmentAverages d ON e.Department = d.Department\n",
    ")\n",
    "\n",
    "SELECT ed.EmployeeID, ed.EmployeeName, ed.Department, ed.Salary\n",
    "FROM EmployeeDetails ed\n",
    "WHERE ed.Salary < ed.dept_avg_salary\n",
    "AND ed.Salary > (SELECT MIN(avg_salary) FROM DepartmentAverages WHERE Department != ed.Department)\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Department|   dept_avg_salary|\n",
      "+----------+------------------+\n",
      "|        HR|            4500.0|\n",
      "|        IT| 6833.333333333333|\n",
      "|   Finance|            5000.0|\n",
      "|     Sales|3233.3333333333335|\n",
      "| Marketing| 4433.333333333333|\n",
      "+----------+------------------+\n",
      "\n",
      "+----------+----------+------------+------+------------------+\n",
      "|Department|EmployeeID|EmployeeName|Salary|   dept_avg_salary|\n",
      "+----------+----------+------------+------+------------------+\n",
      "|        HR|         1|       Alice|  4000|            4500.0|\n",
      "|        HR|         2|         Bob|  5000|            4500.0|\n",
      "|        HR|         3|     Charlie|  4500|            4500.0|\n",
      "|        IT|         4|       David|  6000| 6833.333333333333|\n",
      "|        IT|         5|         Eve|  7500| 6833.333333333333|\n",
      "|        IT|         6|       Frank|  7000| 6833.333333333333|\n",
      "|   Finance|         7|       Grace|  5500|            5000.0|\n",
      "|   Finance|         8|       Heidi|  5000|            5000.0|\n",
      "|   Finance|         9|        Ivan|  4500|            5000.0|\n",
      "|     Sales|        10|        Judy|  3000|3233.3333333333335|\n",
      "|     Sales|        11|       Kevin|  3200|3233.3333333333335|\n",
      "|     Sales|        12|       Laura|  3500|3233.3333333333335|\n",
      "| Marketing|        13|     Mallory|  4000| 4433.333333333333|\n",
      "| Marketing|        14|        Niaj|  4500| 4433.333333333333|\n",
      "| Marketing|        15|       Oscar|  4800| 4433.333333333333|\n",
      "+----------+----------+------------+------+------------------+\n",
      "\n",
      "+----------+----------+------------+------+-----------------+\n",
      "|Department|EmployeeID|EmployeeName|Salary|  dept_avg_salary|\n",
      "+----------+----------+------------+------+-----------------+\n",
      "|        IT|         4|       David|  6000|6833.333333333333|\n",
      "+----------+----------+------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, min, col\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Calculate Department Averages\n",
    "dept_avg_df = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"dept_avg_salary\"))\n",
    "dept_avg_df.show()\n",
    "\n",
    "# Step 2: Calculate the Minimum Average Salary of Other Departments\n",
    "other_dept_avg = dept_avg_df.agg(avg(\"dept_avg_salary\").alias(\"other_dept_avg\")).collect()[0][\"other_dept_avg\"]\n",
    "\n",
    "\n",
    "# Step 3: Join the Employee Data with Department Averages\n",
    "joined_df = df.join(dept_avg_df, \"Department\")\n",
    "joined_df.show()\n",
    "# Step 4: Filter Employees\n",
    "result_df = joined_df.filter((col(\"Salary\") < col(\"dept_avg_salary\")) & (col(\"Salary\") > other_dept_avg))\n",
    "\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+-----------------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|  dept_avg_salary|\n",
      "+----------+------------+----------+------+-----------------+\n",
      "|         9|        Ivan|   Finance|  4500|           5000.0|\n",
      "|         1|       Alice|        HR|  4000|           4500.0|\n",
      "|         4|       David|        IT|  6000|6833.333333333333|\n",
      "|        13|     Mallory| Marketing|  4000|4433.333333333333|\n",
      "+----------+------------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg, min\n",
    "\n",
    "# Step 1: Calculate Department Average Using Window Function\n",
    "dept_avg_window = Window.partitionBy(\"Department\")\n",
    "df = df.withColumn(\"dept_avg_salary\", avg(\"Salary\").over(dept_avg_window))\n",
    "df.show()\n",
    "\n",
    "# Step 2: Calculate Minimum Average of Other Departments\n",
    "other_dept_avg = df.select(\"Department\", \"dept_avg_salary\").distinct()\n",
    "min_other_avg = other_dept_avg.agg(min(\"dept_avg_salary\").alias(\"min_other_avg\")).collect()[0][\"min_other_avg\"]\n",
    "\n",
    "# Step 3: Filter Employees\n",
    "filtered_df = df.filter((col(\"Salary\") < col(\"dept_avg_salary\")) & (col(\"Salary\") > min_other_avg))\n",
    "\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+------+-----------------+-----------------+\n",
      "|Department|EmployeeID|EmployeeName|Salary|  dept_avg_salary|     dept_avg_sal|\n",
      "+----------+----------+------------+------+-----------------+-----------------+\n",
      "|        IT|         4|       David|  6000|6833.333333333333|6833.333333333333|\n",
      "+----------+----------+------------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate Department Average\n",
    "dept_avg = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"dept_avg_sal\"))\n",
    "\n",
    "# Step 2: Calculate the Overall Average Salary\n",
    "overall_avg = df.agg(avg(\"Salary\").alias(\"overall_avg\")).collect()[0][\"overall_avg\"]\n",
    "\n",
    "# Step 3: Join and Filter\n",
    "result_df = df.join(dept_avg, \"Department\") \\\n",
    "    .filter((col(\"Salary\") < col(\"dept_avg_sal\")) & (col(\"Salary\") > overall_avg))\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
