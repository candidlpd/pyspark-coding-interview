{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark-coding-interview'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os       \n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark-coding-interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Version:\n",
      "3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n",
      "\n",
      "Current Python Executable:\n",
      "c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "\n",
      "Installed Python Versions (Using `py` command on Windows):\n",
      "-V:3.11 *        C:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "\n",
      "Python Paths (Using `where` on Windows):\n",
      "c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def get_python_info():\n",
    "    # Method 1: Using sys to get the current Python version and path\n",
    "    current_version = sys.version\n",
    "    current_executable = sys.executable\n",
    "\n",
    "    # Method 2: Using subprocess to list installed Python versions\n",
    "    try:\n",
    "        installed_versions = subprocess.check_output(\"py -0p\", shell=True).decode().strip()\n",
    "    except subprocess.CalledProcessError:\n",
    "        installed_versions = \"Could not retrieve installed versions.\"\n",
    "\n",
    "    # Method 3: Using subprocess to get path from `where` (Windows) or `which` (Linux/Mac)\n",
    "    try:\n",
    "        python_path_windows = subprocess.check_output(\"where python\", shell=True).decode().strip()\n",
    "    except subprocess.CalledProcessError:\n",
    "        python_path_windows = \"Python path not found on Windows.\"\n",
    "\n",
    "  \n",
    "        \n",
    "\n",
    "    # Print all information\n",
    "    print(\"Current Python Version:\")\n",
    "    print(current_version)\n",
    "    print(\"\\nCurrent Python Executable:\")\n",
    "    print(current_executable)\n",
    "    print(\"\\nInstalled Python Versions (Using `py` command on Windows):\")\n",
    "    print(installed_versions)\n",
    "    print(\"\\nPython Paths (Using `where` on Windows):\")\n",
    "    print(python_path_windows)\n",
    "\n",
    "\n",
    "# Call the function to display information\n",
    "get_python_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java Version: openjdk version \"11.0.0.1\" 2023-05-09\n",
      "OpenJDK Runtime Environment 18.9 (build 11.0.0.1+3-5)\n",
      "OpenJDK 64-Bit Server VM 18.9 (build 11.0.0.1+3-5, mixed mode)\n",
      "Java Location: C:\\Program Files\\Java\\jdk-11.0.0.1\\bin\\java.exe\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_java_version():\n",
    "    try:\n",
    "        # Capture the output of the 'java -version' command\n",
    "        java_version = subprocess.run(['java', '-version'], capture_output=True, text=True, check=True).stderr.strip()\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Use 'where' command on Windows or 'which' on Unix-like systems to find Java location\n",
    "        java_location = subprocess.run(['where', 'java'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Java Version: {java_version}\")\n",
    "        print(f\"Java Location: {java_location}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error checking Java version: {e}\")\n",
    "\n",
    "# Example call to the function\n",
    "check_java_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.4.3\n",
      "Spark Location: C:\\spark\\bin\\..\\conf\\;C:\\spark\\jars\\activation-1.1.1.jar;C:\\spark\\jars\\aircompressor-0.21.jar;C:\\spark\\jars\\algebra_2.12-2.0.1.jar;C:\\spark\\jars\\annotations-17.0.0.jar;C:\\spark\\jars\\antlr-runtime-3.5.2.jar;C:\\spark\\jars\\antlr4-runtime-4.9.3.jar;C:\\spark\\jars\\aopalliance-repackaged-2.6.1.jar;C:\\spark\\jars\\arpack-3.0.3.jar;C:\\spark\\jars\\arpack_combined_all-0.1.jar;C:\\spark\\jars\\arrow-format-11.0.0.jar;C:\\spark\\jars\\arrow-memory-core-11.0.0.jar;C:\\spark\\jars\\arrow-memory-netty-11.0.0.jar;C:\\spark\\jars\\arrow-vector-11.0.0.jar;C:\\spark\\jars\\audience-annotations-0.5.0.jar;C:\\spark\\jars\\avro-1.11.1.jar;C:\\spark\\jars\\avro-ipc-1.11.1.jar;C:\\spark\\jars\\avro-mapred-1.11.1.jar;C:\\spark\\jars\\blas-3.0.3.jar;C:\\spark\\jars\\bonecp-0.8.0.RELEASE.jar;C:\\spark\\jars\\breeze-macros_2.12-2.1.0.jar;C:\\spark\\jars\\breeze_2.12-2.1.0.jar;C:\\spark\\jars\\cats-kernel_2.12-2.1.1.jar;C:\\spark\\jars\\chill-java-0.10.0.jar;C:\\spark\\jars\\chill_2.12-0.10.0.jar;C:\\spark\\jars\\commons-cli-1.5.0.jar;C:\\spark\\jars\\commons-codec-1.15.jar;C:\\spark\\jars\\commons-collections-3.2.2.jar;C:\\spark\\jars\\commons-collections4-4.4.jar;C:\\spark\\jars\\commons-compiler-3.1.9.jar;C:\\spark\\jars\\commons-compress-1.22.jar;C:\\spark\\jars\\commons-crypto-1.1.0.jar;C:\\spark\\jars\\commons-dbcp-1.4.jar;C:\\spark\\jars\\commons-io-2.11.0.jar;C:\\spark\\jars\\commons-lang-2.6.jar;C:\\spark\\jars\\commons-lang3-3.12.0.jar;C:\\spark\\jars\\commons-logging-1.1.3.jar;C:\\spark\\jars\\commons-math3-3.6.1.jar;C:\\spark\\jars\\commons-pool-1.5.4.jar;C:\\spark\\jars\\commons-text-1.10.0.jar;C:\\spark\\jars\\compress-lzf-1.1.2.jar;C:\\spark\\jars\\curator-client-2.13.0.jar;C:\\spark\\jars\\curator-framework-2.13.0.jar;C:\\spark\\jars\\curator-recipes-2.13.0.jar;C:\\spark\\jars\\datanucleus-api-jdo-4.2.4.jar;C:\\spark\\jars\\datanucleus-core-4.1.17.jar;C:\\spark\\jars\\datanucleus-rdbms-4.1.19.jar;C:\\spark\\jars\\derby-10.14.2.0.jar;C:\\spark\\jars\\dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar;C:\\spark\\jars\\flatbuffers-java-1.12.0.jar;C:\\spark\\jars\\gson-2.2.4.jar;C:\\spark\\jars\\guava-14.0.1.jar;C:\\spark\\jars\\hadoop-client-api-3.3.4.jar;C:\\spark\\jars\\hadoop-client-runtime-3.3.4.jar;C:\\spark\\jars\\hadoop-shaded-guava-1.1.1.jar;C:\\spark\\jars\\hadoop-yarn-server-web-proxy-3.3.4.jar;C:\\spark\\jars\\HikariCP-2.5.1.jar;C:\\spark\\jars\\hive-beeline-2.3.9.jar;C:\\spark\\jars\\hive-cli-2.3.9.jar;C:\\spark\\jars\\hive-common-2.3.9.jar;C:\\spark\\jars\\hive-exec-2.3.9-core.jar;C:\\spark\\jars\\hive-jdbc-2.3.9.jar;C:\\spark\\jars\\hive-llap-common-2.3.9.jar;C:\\spark\\jars\\hive-metastore-2.3.9.jar;C:\\spark\\jars\\hive-serde-2.3.9.jar;C:\\spark\\jars\\hive-service-rpc-3.1.3.jar;C:\\spark\\jars\\hive-shims-0.23-2.3.9.jar;C:\\spark\\jars\\hive-shims-2.3.9.jar;C:\\spark\\jars\\hive-shims-common-2.3.9.jar;C:\\spark\\jars\\hive-shims-scheduler-2.3.9.jar;C:\\spark\\jars\\hive-storage-api-2.8.1.jar;C:\\spark\\jars\\hk2-api-2.6.1.jar;C:\\spark\\jars\\hk2-locator-2.6.1.jar;C:\\spark\\jars\\hk2-utils-2.6.1.jar;C:\\spark\\jars\\httpclient-4.5.14.jar;C:\\spark\\jars\\httpcore-4.4.16.jar;C:\\spark\\jars\\istack-commons-runtime-3.0.8.jar;C:\\spark\\jars\\ivy-2.5.1.jar;C:\\spark\\jars\\jackson-annotations-2.14.2.jar;C:\\spark\\jars\\jackson-core-2.14.2.jar;C:\\spark\\jars\\jackson-core-asl-1.9.13.jar;C:\\spark\\jars\\jackson-databind-2.14.2.jar;C:\\spark\\jars\\jackson-dataformat-yaml-2.14.2.jar;C:\\spark\\jars\\jackson-datatype-jsr310-2.14.2.jar;C:\\spark\\jars\\jackson-mapper-asl-1.9.13.jar;C:\\spark\\jars\\jackson-module-scala_2.12-2.14.2.jar;C:\\spark\\jars\\jakarta.annotation-api-1.3.5.jar;C:\\spark\\jars\\jakarta.inject-2.6.1.jar;C:\\spark\\jars\\jakarta.servlet-api-4.0.3.jar;C:\\spark\\jars\\jakarta.validation-api-2.0.2.jar;C:\\spark\\jars\\jakarta.ws.rs-api-2.1.6.jar;C:\\spark\\jars\\jakarta.xml.bind-api-2.3.2.jar;C:\\spark\\jars\\janino-3.1.9.jar;C:\\spark\\jars\\javassist-3.25.0-GA.jar;C:\\spark\\jars\\javax.jdo-3.2.0-m3.jar;C:\\spark\\jars\\javolution-5.5.1.jar;C:\\spark\\jars\\jaxb-runtime-2.3.2.jar;C:\\spark\\jars\\jcl-over-slf4j-2.0.6.jar;C:\\spark\\jars\\jdo-api-3.0.1.jar;C:\\spark\\jars\\jersey-client-2.36.jar;C:\\spark\\jars\\jersey-common-2.36.jar;C:\\spark\\jars\\jersey-container-servlet-2.36.jar;C:\\spark\\jars\\jersey-container-servlet-core-2.36.jar;C:\\spark\\jars\\jersey-hk2-2.36.jar;C:\\spark\\jars\\jersey-server-2.36.jar;C:\\spark\\jars\\JLargeArrays-1.5.jar;C:\\spark\\jars\\jline-2.14.6.jar;C:\\spark\\jars\\joda-time-2.12.2.jar;C:\\spark\\jars\\jodd-core-3.5.2.jar;C:\\spark\\jars\\jpam-1.1.jar;C:\\spark\\jars\\json-1.8.jar;C:\\spark\\jars\\json4s-ast_2.12-3.7.0-M11.jar;C:\\spark\\jars\\json4s-core_2.12-3.7.0-M11.jar;C:\\spark\\jars\\json4s-jackson_2.12-3.7.0-M11.jar;C:\\spark\\jars\\json4s-scalap_2.12-3.7.0-M11.jar;C:\\spark\\jars\\jsr305-3.0.0.jar;C:\\spark\\jars\\jta-1.1.jar;C:\\spark\\jars\\JTransforms-3.1.jar;C:\\spark\\jars\\jul-to-slf4j-2.0.6.jar;C:\\spark\\jars\\kryo-shaded-4.0.2.jar;C:\\spark\\jars\\kubernetes-client-6.4.1.jar;C:\\spark\\jars\\kubernetes-client-api-6.4.1.jar;C:\\spark\\jars\\kubernetes-httpclient-okhttp-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-admissionregistration-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-apiextensions-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-apps-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-autoscaling-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-batch-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-certificates-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-common-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-coordination-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-core-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-discovery-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-events-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-extensions-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-flowcontrol-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-gatewayapi-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-metrics-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-networking-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-node-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-policy-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-rbac-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-scheduling-6.4.1.jar;C:\\spark\\jars\\kubernetes-model-storageclass-6.4.1.jar;C:\\spark\\jars\\lapack-3.0.3.jar;C:\\spark\\jars\\leveldbjni-all-1.8.jar;C:\\spark\\jars\\libfb303-0.9.3.jar;C:\\spark\\jars\\libthrift-0.12.0.jar;C:\\spark\\jars\\log4j-1.2-api-2.19.0.jar;C:\\spark\\jars\\log4j-api-2.19.0.jar;C:\\spark\\jars\\log4j-core-2.19.0.jar;C:\\spark\\jars\\log4j-slf4j2-impl-2.19.0.jar;C:\\spark\\jars\\logging-interceptor-3.12.12.jar;C:\\spark\\jars\\lz4-java-1.8.0.jar;C:\\spark\\jars\\mesos-1.4.3-shaded-protobuf.jar;C:\\spark\\jars\\metrics-core-4.2.15.jar;C:\\spark\\jars\\metrics-graphite-4.2.15.jar;C:\\spark\\jars\\metrics-jmx-4.2.15.jar;C:\\spark\\jars\\metrics-json-4.2.15.jar;C:\\spark\\jars\\metrics-jvm-4.2.15.jar;C:\\spark\\jars\\minlog-1.3.0.jar;C:\\spark\\jars\\netty-all-4.1.87.Final.jar;C:\\spark\\jars\\netty-buffer-4.1.87.Final.jar;C:\\spark\\jars\\netty-codec-4.1.87.Final.jar;C:\\spark\\jars\\netty-codec-http-4.1.87.Final.jar;C:\\spark\\jars\\netty-codec-http2-4.1.87.Final.jar;C:\\spark\\jars\\netty-codec-socks-4.1.87.Final.jar;C:\\spark\\jars\\netty-common-4.1.87.Final.jar;C:\\spark\\jars\\netty-handler-4.1.87.Final.jar;C:\\spark\\jars\\netty-handler-proxy-4.1.87.Final.jar;C:\\spark\\jars\\netty-resolver-4.1.87.Final.jar;C:\\spark\\jars\\netty-transport-4.1.87.Final.jar;C:\\spark\\jars\\netty-transport-classes-epoll-4.1.87.Final.jar;C:\\spark\\jars\\netty-transport-classes-kqueue-4.1.87.Final.jar;C:\\spark\\jars\\netty-transport-native-epoll-4.1.87.Final-linux-aarch_64.jar;C:\\spark\\jars\\netty-transport-native-epoll-4.1.87.Final-linux-x86_64.jar;C:\\spark\\jars\\netty-transport-native-kqueue-4.1.87.Final-osx-aarch_64.jar;C:\\spark\\jars\\netty-transport-native-kqueue-4.1.87.Final-osx-x86_64.jar;C:\\spark\\jars\\netty-transport-native-unix-common-4.1.87.Final.jar;C:\\spark\\jars\\objenesis-3.2.jar;C:\\spark\\jars\\okhttp-3.12.12.jar;C:\\spark\\jars\\okio-1.15.0.jar;C:\\spark\\jars\\opencsv-2.3.jar;C:\\spark\\jars\\orc-core-1.8.7-shaded-protobuf.jar;C:\\spark\\jars\\orc-mapreduce-1.8.7-shaded-protobuf.jar;C:\\spark\\jars\\orc-shims-1.8.7.jar;C:\\spark\\jars\\oro-2.0.8.jar;C:\\spark\\jars\\osgi-resource-locator-1.0.3.jar;C:\\spark\\jars\\paranamer-2.8.jar;C:\\spark\\jars\\parquet-column-1.12.3.jar;C:\\spark\\jars\\parquet-common-1.12.3.jar;C:\\spark\\jars\\parquet-encoding-1.12.3.jar;C:\\spark\\jars\\parquet-format-structures-1.12.3.jar;C:\\spark\\jars\\parquet-hadoop-1.12.3.jar;C:\\spark\\jars\\parquet-jackson-1.12.3.jar;C:\\spark\\jars\\pickle-1.3.jar;C:\\spark\\jars\\protobuf-java-2.5.0.jar;C:\\spark\\jars\\py4j-0.10.9.7.jar;C:\\spark\\jars\\RoaringBitmap-0.9.38.jar;C:\\spark\\jars\\rocksdbjni-7.9.2.jar;C:\\spark\\jars\\scala-collection-compat_2.12-2.7.0.jar;C:\\spark\\jars\\scala-compiler-2.12.17.jar;C:\\spark\\jars\\scala-library-2.12.17.jar;C:\\spark\\jars\\scala-parser-combinators_2.12-2.1.1.jar;C:\\spark\\jars\\scala-reflect-2.12.17.jar;C:\\spark\\jars\\scala-xml_2.12-2.1.0.jar;C:\\spark\\jars\\shims-0.9.38.jar;C:\\spark\\jars\\slf4j-api-2.0.6.jar;C:\\spark\\jars\\snakeyaml-1.33.jar;C:\\spark\\jars\\snappy-java-1.1.10.5.jar;C:\\spark\\jars\\spark-catalyst_2.12-3.4.3.jar;C:\\spark\\jars\\spark-core_2.12-3.4.3.jar;C:\\spark\\jars\\spark-graphx_2.12-3.4.3.jar;C:\\spark\\jars\\spark-hive-thriftserver_2.12-3.4.3.jar;C:\\spark\\jars\\spark-hive_2.12-3.4.3.jar;C:\\spark\\jars\\spark-kubernetes_2.12-3.4.3.jar;C:\\spark\\jars\\spark-kvstore_2.12-3.4.3.jar;C:\\spark\\jars\\spark-launcher_2.12-3.4.3.jar;C:\\spark\\jars\\spark-mesos_2.12-3.4.3.jar;C:\\spark\\jars\\spark-mllib-local_2.12-3.4.3.jar;C:\\spark\\jars\\spark-mllib_2.12-3.4.3.jar;C:\\spark\\jars\\spark-network-common_2.12-3.4.3.jar;C:\\spark\\jars\\spark-network-shuffle_2.12-3.4.3.jar;C:\\spark\\jars\\spark-repl_2.12-3.4.3.jar;C:\\spark\\jars\\spark-sketch_2.12-3.4.3.jar;C:\\spark\\jars\\spark-sql_2.12-3.4.3.jar;C:\\spark\\jars\\spark-streaming_2.12-3.4.3.jar;C:\\spark\\jars\\spark-tags_2.12-3.4.3.jar;C:\\spark\\jars\\spark-unsafe_2.12-3.4.3.jar;C:\\spark\\jars\\spark-yarn_2.12-3.4.3.jar;C:\\spark\\jars\\spire-macros_2.12-0.17.0.jar;C:\\spark\\jars\\spire-platform_2.12-0.17.0.jar;C:\\spark\\jars\\spire-util_2.12-0.17.0.jar;C:\\spark\\jars\\spire_2.12-0.17.0.jar;C:\\spark\\jars\\ST4-4.0.4.jar;C:\\spark\\jars\\stax-api-1.0.1.jar;C:\\spark\\jars\\stream-2.9.6.jar;C:\\spark\\jars\\super-csv-2.2.0.jar;C:\\spark\\jars\\threeten-extra-1.7.1.jar;C:\\spark\\jars\\tink-1.7.0.jar;C:\\spark\\jars\\transaction-api-1.1.jar;C:\\spark\\jars\\univocity-parsers-2.9.1.jar;C:\\spark\\jars\\xbean-asm9-shaded-4.22.jar;C:\\spark\\jars\\xz-1.9.jar;C:\\spark\\jars\\zjsonpatch-0.3.0.jar;C:\\spark\\jars\\zookeeper-3.6.3.jar;C:\\spark\\jars\\zookeeper-jute-3.6.3.jar;C:\\spark\\jars\n"
     ]
    }
   ],
   "source": [
    "def check_spark_version():\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        spark_version = spark.version\n",
    "        spark_location = os.path.dirname(spark.sparkContext._jvm.java.lang.System.getProperty(\"java.class.path\"))\n",
    "        print(f\"Spark Version: {spark_version}\")\n",
    "        print(f\"Spark Location: {spark_location}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking Spark version: {e}\")\n",
    "        \n",
    "check_spark_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version: 3.4.3\n",
      "PySpark Location: C:\\spark\\python\\pyspark\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "def check_pyspark_version():\n",
    "    try:\n",
    "        import pyspark\n",
    "        \n",
    "        print(f\"PySpark Version: {pyspark.__version__}\")\n",
    "        print(f\"PySpark Location: {pyspark.__file__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error checking PySpark version: {e}\")\n",
    "        \n",
    "check_pyspark_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World: 1\n",
      "Spark: 1\n",
      "Welcome: 1\n",
      "to: 1\n",
      "Hello: 2\n",
      "PySpark: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WordCountExample\").getOrCreate()\n",
    "\n",
    "# Create an RDD\n",
    "data = [\"Hello World\", \"Hello PySpark\", \"Welcome to Spark\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform a word count\n",
    "word_counts = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and display results\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 27|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkTest\").getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 27)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor name: 12th Gen Intel(R) Core(TM) i7-12700H\n",
      "Total Physical Cores: 14\n",
      "Total Logical Cores (Threads): 14\n",
      "Total Memory: 31.72 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "\n",
    "processor_info = os.popen(\"wmic cpu get name\").read().strip().split(\"\\n\")\n",
    "processor_name = [line.strip() for line in processor_info if line.strip()][1]  # Ensure to pick the second non-empty line\n",
    "print(f'Processor name: {processor_name}')\n",
    "\n",
    "\n",
    "print(f\"Total Physical Cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Total Logical Cores (Threads): {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Total Memory: {psutil.virtual_memory().total / (1024**3):.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 1.0%\n",
      "Total Memory: 31.72 GB\n",
      "Available Memory: 15.98 GB\n",
      "Memory Usage: 49.6%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Get CPU usage percentage\n",
    "cpu_usage = psutil.cpu_percent(interval=1)\n",
    "print(f\"CPU Usage: {cpu_usage}%\")\n",
    "\n",
    "# Get memory usage information\n",
    "\n",
    "print(f\"Total Memory: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Memory Usage: {psutil.virtual_memory().percent}%\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.submitTime', '1729286358093'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.app.id', 'local-1729288460320'), ('spark.app.startTime', '1729288460278'), ('spark.sql.warehouse.dir', 'file:/C:/Users/lpdda/AppData/Local/Programs/Microsoft%20VS%20Code/spark-warehouse'), ('spark.driver.port', '61666'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'RemoveLeadingTrailingZeros'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "# Check Configuration\n",
    "print(sc.getConf().getAll())\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|original_string|cleaned_string|\n",
      "+---------------+--------------+\n",
      "|        0012300|           123|\n",
      "|       00040050|          4005|\n",
      "|           0500|             5|\n",
      "|           0000|              |\n",
      "|          12345|         12345|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveLeadingTrailingZeros\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"0012300\",), (\"00040050\",), (\"0500\",), (\"0000\",), (\"12345\",)]\n",
    "columns = [\"original_string\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "#spark.sql(\"select * from temp_table\").show()\n",
    "\n",
    "res = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        original_string,\n",
    "        regexp_replace(original_string, '^0+|0+$', '') AS cleaned_string\n",
    "    FROM temp_table\n",
    "\"\"\")\n",
    "\n",
    "res.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
