{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark-coding-interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os       \n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark-coding-interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.startTime', '1729289650138'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.warehouse.dir', 'file:/H:/pyspark-coding-interview/spark-warehouse'), ('spark.app.id', 'local-1729289650182'), ('spark.driver.port', '63286'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.executor.cores', '4'), ('spark.app.submitTime', '1729289495517'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.sql.shuffle.partitions', '28'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.pyFiles', ''), ('spark.app.name', 'OptimizedLocalSpark'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.cores.max', '12')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "# Check Configuration\n",
    "print(sc.getConf().getAll())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|alphanumeric_string|\n",
      "+-------------------+\n",
      "|       abc123xyz456|\n",
      "|            test789|\n",
      "|           hello123|\n",
      "|                abc|\n",
      "|                123|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"abc123xyz456\",), (\"test789\",), (\"hello123\",), (\"abc\",), (\"123\",)]\n",
    "columns = [\"alphanumeric_string\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.cache()\n",
    "\n",
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"alphanumeric_table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+\n",
      "|alphanumeric_string|numbers|alphabets|\n",
      "+-------------------+-------+---------+\n",
      "|       abc123xyz456| 123456|   abcxyz|\n",
      "|            test789|    789|     test|\n",
      "|           hello123|    123|    hello|\n",
      "|                abc|       |      abc|\n",
      "|                123|    123|         |\n",
      "+-------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        alphanumeric_string,\n",
    "        regexp_replace(alphanumeric_string, '[^0-9]', '') AS numbers,\n",
    "        regexp_replace(alphanumeric_string, '[^a-zA-Z]', '') AS alphabets\n",
    "    FROM alphanumeric_table\n",
    "\"\"\")\n",
    "\n",
    "# Show result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+\n",
      "|alphanumeric_string|numbers|alphabets|\n",
      "+-------------------+-------+---------+\n",
      "|       abc123xyz456|    123|      abc|\n",
      "|            test789|    789|     test|\n",
      "|           hello123|    123|    hello|\n",
      "|                abc|       |      abc|\n",
      "|                123|    123|         |\n",
      "+-------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        alphanumeric_string, \n",
    "        regexp_extract(alphanumeric_string, '[0-9]+', 0) AS numbers,\n",
    "        regexp_extract(alphanumeric_string, '[a-zA-Z]+', 0) AS alphabets\n",
    "    FROM alphanumeric_table\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "res2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+\n",
      "|alphanumeric_string|numbers|alphabets|\n",
      "+-------------------+-------+---------+\n",
      "|       abc123xyz456| 123456|   abcxyz|\n",
      "|            test789|    789|     test|\n",
      "|           hello123|    123|    hello|\n",
      "|                abc|       |      abc|\n",
      "|                123|    123|         |\n",
      "+-------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "df1 = df.withColumn(\"numbers\", regexp_replace(\"alphanumeric_string\", \"[^0-9]\", \"\")).withColumn(\"alphabets\", regexp_replace(\"alphanumeric_string\", \"[^a-zA-Z]\", \"\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------+\n",
      "|alphanumeric_string|only_numbers|only_alphabets|\n",
      "+-------------------+------------+--------------+\n",
      "|       abc123xyz456|         123|           abc|\n",
      "|            test789|         789|          test|\n",
      "|           hello123|         123|         hello|\n",
      "|                abc|            |           abc|\n",
      "|                123|         123|              |\n",
      "+-------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"only_numbers\", regexp_extract(\"alphanumeric_string\", \"[0-9]+\", 0)).withColumn(\"only_alphabets\", regexp_extract(\"alphanumeric_string\", \"[a-zA-Z]+\", 0))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------+\n",
      "|alphanumeric_string|numbers_udf|alphabets_udf|\n",
      "+-------------------+-----------+-------------+\n",
      "|       abc123xyz456|     123456|       abcxyz|\n",
      "|            test789|        789|         test|\n",
      "|           hello123|        123|        hello|\n",
      "|                abc|           |          abc|\n",
      "|                123|        123|             |\n",
      "+-------------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# Define UDFs\n",
    "def extract_numbers(s):\n",
    "    return \"\".join(re.findall(r\"[0-9]+\", s))\n",
    "\n",
    "def extract_alphabets(s):\n",
    "    return \"\".join(re.findall(r\"[a-zA-Z]+\", s))\n",
    "\n",
    "# Register UDFs\n",
    "extract_numbers_udf = udf(extract_numbers, StringType())\n",
    "extract_alphabets_udf = udf(extract_alphabets, StringType())\n",
    "\n",
    "# Apply UDFs to DataFrame\n",
    "df = df.withColumn(\"numbers_udf\", extract_numbers_udf(\"alphanumeric_string\"))\n",
    "df = df.withColumn(\"alphabets_udf\", extract_alphabets_udf(\"alphanumeric_string\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
