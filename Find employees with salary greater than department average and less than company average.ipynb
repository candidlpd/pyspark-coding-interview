{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         1|       Alice|        HR|  4000|\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         3|     Charlie|        HR|  4500|\n",
      "|         4|       David|        IT|  6000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|         6|       Frank|        IT|  7000|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|         8|       Heidi|   Finance|  5000|\n",
      "|         9|        Ivan|   Finance|  4500|\n",
      "|        10|        Judy|     Sales|  3000|\n",
      "|        11|       Kevin|     Sales|  3200|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        13|     Mallory| Marketing|  4000|\n",
      "|        14|        Niaj| Marketing|  4500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Sample employee data\n",
    "data = [\n",
    "    Row(EmployeeID=1, EmployeeName=\"Alice\", Department=\"HR\", Salary=4000),\n",
    "    Row(EmployeeID=2, EmployeeName=\"Bob\", Department=\"HR\", Salary=5000),\n",
    "    Row(EmployeeID=3, EmployeeName=\"Charlie\", Department=\"HR\", Salary=4500),\n",
    "    Row(EmployeeID=4, EmployeeName=\"David\", Department=\"IT\", Salary=6000),\n",
    "    Row(EmployeeID=5, EmployeeName=\"Eve\", Department=\"IT\", Salary=7500),\n",
    "    Row(EmployeeID=6, EmployeeName=\"Frank\", Department=\"IT\", Salary=7000),\n",
    "    Row(EmployeeID=7, EmployeeName=\"Grace\", Department=\"Finance\", Salary=5500),\n",
    "    Row(EmployeeID=8, EmployeeName=\"Heidi\", Department=\"Finance\", Salary=5000),\n",
    "    Row(EmployeeID=9, EmployeeName=\"Ivan\", Department=\"Finance\", Salary=4500),\n",
    "    Row(EmployeeID=10, EmployeeName=\"Judy\", Department=\"Sales\", Salary=3000),\n",
    "    Row(EmployeeID=11, EmployeeName=\"Kevin\", Department=\"Sales\", Salary=3200),\n",
    "    Row(EmployeeID=12, EmployeeName=\"Laura\", Department=\"Sales\", Salary=3500),\n",
    "    Row(EmployeeID=13, EmployeeName=\"Mallory\", Department=\"Marketing\", Salary=4000),\n",
    "    Row(EmployeeID=14, EmployeeName=\"Niaj\", Department=\"Marketing\", Salary=4500),\n",
    "    Row(EmployeeID=15, EmployeeName=\"Oscar\", Department=\"Marketing\", Salary=4800)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "df.cache()\n",
    "# Display the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        14|        Niaj| Marketing|  4500|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "# SQL Query to Find Employees Based on the Criteria\n",
    "res5 = spark.sql( \"\"\"\n",
    "WITH DepartmentAverages AS (\n",
    "    SELECT Department, AVG(Salary) AS Dept_Avg_Salary \n",
    "    FROM Employees \n",
    "    GROUP BY Department\n",
    "),\n",
    "\n",
    "\n",
    "CompanyAverage AS (\n",
    "    SELECT AVG(Salary) AS Company_Avg_Salary FROM Employees\n",
    ")\n",
    "\n",
    "\n",
    "SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary\n",
    "FROM Employees e\n",
    "JOIN DepartmentAverages d ON e.Department = d.Department\n",
    "JOIN CompanyAverage c\n",
    "WHERE e.Salary > d.Dept_Avg_Salary \n",
    "AND e.Salary < c.Company_Avg_Salary\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "res5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+------------------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|        avg_salary|\n",
      "+----------+------------+----------+------+------------------+\n",
      "|         2|         Bob|        HR|  5000|            4500.0|\n",
      "|         6|       Frank|        IT|  7000| 6833.333333333333|\n",
      "|         5|         Eve|        IT|  7500| 6833.333333333333|\n",
      "|         7|       Grace|   Finance|  5500|            5000.0|\n",
      "|        12|       Laura|     Sales|  3500|3233.3333333333335|\n",
      "|        15|       Oscar| Marketing|  4800| 4433.333333333333|\n",
      "|        14|        Niaj| Marketing|  4500| 4433.333333333333|\n",
      "+----------+------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Employees having greater than average salary of the department\n",
    "\n",
    "\n",
    "res = spark.sql(\"\"\"\n",
    "   SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary,avgSalEmp.avg_salary from Employees  e   \n",
    "   inner join      \n",
    "   (select  Department, avg(Salary) as avg_salary from Employees group by  Department) avgSalEmp\n",
    "   on e.Department = avgSalEmp. Department\n",
    "   and e.Salary > avgSalEmp.avg_salary\n",
    "                \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+------------------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|        avg_salary|\n",
      "+----------+------------+----------+------+------------------+\n",
      "|        12|       Laura|     Sales|  3500|3233.3333333333335|\n",
      "|        14|        Niaj| Marketing|  4500| 4433.333333333333|\n",
      "+----------+------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Employees having greater than average salary of the department but less than the overall average \n",
    "res1 = spark.sql(\"\"\"\n",
    "   SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary,avgSalEmp.avg_salary from Employees  e   \n",
    "   inner join      \n",
    "   (select  Department, avg(Salary) as avg_salary from Employees group by  Department) avgSalEmp\n",
    "   on e.Department = avgSalEmp. Department\n",
    "   and e.Salary > avgSalEmp.avg_salary\n",
    "   where e.Salary < (select avg(Salary) from Employees)\n",
    "                \n",
    "                \"\"\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+------+------------------+\n",
      "|Department|EmployeeID|EmployeeName|Salary|   Dept_Avg_Salary|\n",
      "+----------+----------+------------+------+------------------+\n",
      "|     Sales|        12|       Laura|  3500|3233.3333333333335|\n",
      "| Marketing|        14|        Niaj|  4500| 4433.333333333333|\n",
      "+----------+----------+------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "# Step 1: Calculate the Department Average Salary\n",
    "dept_avg_df = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Dept_Avg_Salary\"))\n",
    "\n",
    "# Step 2: Calculate the Company Average Salary\n",
    "company_avg = df.agg(avg(\"Salary\").alias(\"Company_Avg_Salary\")).collect()[0][\"Company_Avg_Salary\"]\n",
    "\n",
    "# Step 3: Join the Employee Data with Department Averages\n",
    "joined_df = df.join(dept_avg_df, \"Department\")\n",
    "\n",
    "# Step 4: Filter Employees\n",
    "result_df = joined_df.filter((col(\"Salary\") > col(\"Dept_Avg_Salary\")) & (col(\"Salary\") < company_avg))\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+------------------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|   Dept_Avg_Salary|\n",
      "+----------+------------+----------+------+------------------+\n",
      "|        14|        Niaj| Marketing|  4500| 4433.333333333333|\n",
      "|        12|       Laura|     Sales|  3500|3233.3333333333335|\n",
      "+----------+------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Step 1: Calculate Department Average Using Window Function\n",
    "dept_avg_window = Window.partitionBy(\"Department\")\n",
    "df = df.withColumn(\"Dept_Avg_Salary\", avg(\"Salary\").over(dept_avg_window))\n",
    "\n",
    "# Step 2: Calculate Company Average\n",
    "company_avg = df.agg(avg(\"Salary\").alias(\"Company_Avg_Salary\")).collect()[0][\"Company_Avg_Salary\"]\n",
    "\n",
    "# Step 3: Filter Employees\n",
    "result_window_df = df.filter((col(\"Salary\") > col(\"Dept_Avg_Salary\")) & (col(\"Salary\") < company_avg))\n",
    "\n",
    "result_window_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+------+------------------+------------------+\n",
      "|Department|EmployeeID|EmployeeName|Salary|   Dept_Avg_Salary|      Dept_Avg_Sal|\n",
      "+----------+----------+------------+------+------------------+------------------+\n",
      "| Marketing|        14|        Niaj|  4500| 4433.333333333333| 4433.333333333333|\n",
      "|     Sales|        12|       Laura|  3500|3233.3333333333335|3233.3333333333335|\n",
      "+----------+----------+------------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate Department and Company Averages Separately\n",
    "dept_avg = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Dept_Avg_Sal\"))\n",
    "company_avg = df.agg(avg(\"Salary\").alias(\"Company_Avg_Salary\")).collect()[0][\"Company_Avg_Salary\"]\n",
    "\n",
    "# Step 2: Join the Department Averages with Employee Data\n",
    "joined_avg_df = df.join(dept_avg, \"Department\")\n",
    "\n",
    "# Step 3: Apply Filtering Criteria\n",
    "filtered_result = joined_avg_df.filter((col(\"Salary\") > col(\"Dept_Avg_Sal\")) & (col(\"Salary\") < company_avg))\n",
    "\n",
    "filtered_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
