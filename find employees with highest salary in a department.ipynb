{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         1|       Alice|        HR|  4000|\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         3|     Charlie|        HR|  4500|\n",
      "|         4|       David|        IT|  6000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|         6|       Frank|        IT|  7000|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|         8|       Heidi|   Finance|  5000|\n",
      "|         9|        Ivan|   Finance|  4500|\n",
      "|        10|        Judy|     Sales|  3000|\n",
      "|        11|       Kevin|     Sales|  3200|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        13|     Mallory| Marketing|  4000|\n",
      "|        14|        Niaj| Marketing|  4500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Sample employee data\n",
    "data = [\n",
    "    Row(EmployeeID=1, EmployeeName=\"Alice\", Department=\"HR\", Salary=4000),\n",
    "    Row(EmployeeID=2, EmployeeName=\"Bob\", Department=\"HR\", Salary=5000),\n",
    "    Row(EmployeeID=3, EmployeeName=\"Charlie\", Department=\"HR\", Salary=4500),\n",
    "    Row(EmployeeID=4, EmployeeName=\"David\", Department=\"IT\", Salary=6000),\n",
    "    Row(EmployeeID=5, EmployeeName=\"Eve\", Department=\"IT\", Salary=7500),\n",
    "    Row(EmployeeID=6, EmployeeName=\"Frank\", Department=\"IT\", Salary=7000),\n",
    "    Row(EmployeeID=7, EmployeeName=\"Grace\", Department=\"Finance\", Salary=5500),\n",
    "    Row(EmployeeID=8, EmployeeName=\"Heidi\", Department=\"Finance\", Salary=5000),\n",
    "    Row(EmployeeID=9, EmployeeName=\"Ivan\", Department=\"Finance\", Salary=4500),\n",
    "    Row(EmployeeID=10, EmployeeName=\"Judy\", Department=\"Sales\", Salary=3000),\n",
    "    Row(EmployeeID=11, EmployeeName=\"Kevin\", Department=\"Sales\", Salary=3200),\n",
    "    Row(EmployeeID=12, EmployeeName=\"Laura\", Department=\"Sales\", Salary=3500),\n",
    "    Row(EmployeeID=13, EmployeeName=\"Mallory\", Department=\"Marketing\", Salary=4000),\n",
    "    Row(EmployeeID=14, EmployeeName=\"Niaj\", Department=\"Marketing\", Salary=4500),\n",
    "    Row(EmployeeID=15, EmployeeName=\"Oscar\", Department=\"Marketing\", Salary=4800)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "df.cache()\n",
    "# Display the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+----------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|max_salary|\n",
      "+----------+------------+----------+------+----------+\n",
      "|         2|         Bob|        HR|  5000|      5000|\n",
      "|         5|         Eve|        IT|  7500|      7500|\n",
      "|         7|       Grace|   Finance|  5500|      5500|\n",
      "|        12|       Laura|     Sales|  3500|      3500|\n",
      "|        15|       Oscar| Marketing|  4800|      4800|\n",
      "+----------+------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find employees with highest salary in a department\n",
    "res = spark.sql(\"\"\"\n",
    "   SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary,maxSalEmp.max_salary from Employees  e   \n",
    "   inner join      \n",
    "   (select  Department, Max(Salary) as max_salary from Employees group by  Department) maxSalEmp\n",
    "   on e.Department = maxSalEmp. Department\n",
    "   and e.Salary = maxSalEmp.max_salary\n",
    "                \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res3= spark.sql(\"\"\"\n",
    "SELECT e.EmployeeID, e.EmployeeName, e.Department, e.Salary\n",
    "FROM Employees e\n",
    "WHERE e.Salary = (\n",
    "    SELECT MAX(Salary) \n",
    "    FROM Employees \n",
    "    WHERE Department = e.Department\n",
    ")\n",
    "\"\"\" )\n",
    "res3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+--------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|rank_sal|\n",
      "+----------+------------+----------+------+--------+\n",
      "|         7|       Grace|   Finance|  5500|       1|\n",
      "|         2|         Bob|        HR|  5000|       1|\n",
      "|         5|         Eve|        IT|  7500|       1|\n",
      "|        15|       Oscar| Marketing|  4800|       1|\n",
      "|        12|       Laura|     Sales|  3500|       1|\n",
      "+----------+------------+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\"\n",
    "    select * from (\n",
    "   SELECT EmployeeID, EmployeeName, Department, Salary,\n",
    "   rank() over (partition by Department order by Salary desc) as rank_sal\n",
    "   from Employees ) as emp \n",
    "   where emp.rank_sal = 1\n",
    "  \n",
    "    \"\"\")\n",
    "res2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "# Define a Window Specification partitioned by Department and ordered by Salary in descending order\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Add row numbers to identify the highest salary in each department\n",
    "highest_salary_df = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .select(\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\")\n",
    "\n",
    "# Show the result\n",
    "highest_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Define a Window Specification partitioned by Department and ordered by Salary in descending order\n",
    "window_spec_rank = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Add ranks to identify the highest salary in each department\n",
    "ranked_salary_df = df.withColumn(\"rank\", dense_rank().over(window_spec_rank)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\")\n",
    "\n",
    "# Show the result\n",
    "ranked_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+----+\n",
      "|EmployeeID|EmployeeName|Department|Salary|rank|\n",
      "+----------+------------+----------+------+----+\n",
      "|         7|       Grace|   Finance|  5500|5500|\n",
      "|         2|         Bob|        HR|  5000|5000|\n",
      "|         5|         Eve|        IT|  7500|7500|\n",
      "|        15|       Oscar| Marketing|  4800|4800|\n",
      "|        12|       Laura|     Sales|  3500|3500|\n",
      "+----------+------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "# Define a window to partition by Department and order by Salary in descending order\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Use window function to rank salaries within each department\n",
    "df_with_rank = df.withColumn(\"rank\", max(\"Salary\").over(window_spec))\n",
    "\n",
    "# Filter to keep only employees with the highest salary (rank = 1)\n",
    "highest_salary_df = df_with_rank.filter(col(\"rank\") == col(\"Salary\"))\n",
    "\n",
    "# Show the result\n",
    "highest_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         2|         Bob|        HR|  5000|\n",
      "|         5|         Eve|        IT|  7500|\n",
      "|         7|       Grace|   Finance|  5500|\n",
      "|        12|       Laura|     Sales|  3500|\n",
      "|        15|       Oscar| Marketing|  4800|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "# Step 1: Calculate maximum salary for each department\n",
    "dept_max_salary = df.groupBy(\"Department\").agg(max(\"Salary\").alias(\"MaxSalary\"))\n",
    "\n",
    "# Step 2: Alias the original DataFrame and the max salary DataFrame\n",
    "df_alias = df.alias(\"e\")\n",
    "dept_max_salary_alias = dept_max_salary.alias(\"d\")\n",
    "\n",
    "# Step 3: Join with original DataFrame to find employees with the max salary\n",
    "highest_salary_df = df_alias.join(\n",
    "    dept_max_salary_alias, \n",
    "    (df_alias.Department == dept_max_salary_alias.Department) & \n",
    "    (df_alias.Salary == dept_max_salary_alias.MaxSalary)\n",
    ").select(\n",
    "    df_alias.EmployeeID, \n",
    "    df_alias.EmployeeName, \n",
    "    df_alias.Department, \n",
    "    df_alias.Salary\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "highest_salary_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
